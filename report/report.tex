\documentclass{report}

\usepackage[margin=3cm]{geometry}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{caption} 
\usepackage{subcaption}
\captionsetup[table]{skip=0pt}
\usepackage{subcaption}
\usepackage{url}
\usepackage{hhline}

\usepackage{algorithm}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Random Walks course project\\Simulated Annealing Algorithm for Graph Coloring}

\author{
  R\'oger Berm\'udez Chac\'on\\EPFL
  \and
  Victor Kristof\\EPFL
  \and
  Merlin Nimier-David\\EPFL
}

\begin{document}
  \maketitle

  \paragraph{Abstract}
  We formulate the graph coloring problem as an optimization problem and use the Monte Carlo Markov Chain method to find proper colorings. Simulated annealing is used to improve the convergence speed. In this report, we recall the problem statement and formulation, present the simulated annealing schemes devised, give relevant implementation details and finally present results.

  % -----------------------------------------------------------------------------------
  \section*{Problem statement}
  \paragraph{Proper $q$-colorings}
  Let $G = (V, E)$ a graph with $|V| = N$. A \emph{coloring} using $q$ colors is an assignment $x \in \{ 1, \ldots, q \}^N$. A \emph{proper} $q$-coloring is a coloring such that no two adjacent edges have the same color: $(v, v') \in E \Rightarrow x_v \neq x_{v'}$. From this definition, we can define a cost function (Hamiltonian) to evaluate the ``quality'' of a given coloring:
  \[
    H(x) = \sum_{(v, v') \in E} 1_{x_v = x_{v'}}
  \]

  In the general case, finding the proper $q$-coloring of a graph of size $N$ is NP complete\footnote{\url{https://en.wikipedia.org/wiki/Graph_coloring#Computational_complexity}}, assuming $2 < q < N$. The search space is the domain of all colorings $x$, which has a size exponential in $N$. It is therefore interesting to relax the problem into an optimization problem. Using the cost function defined above:
  \[
    x^* = \argmin_{x \in \{ 1, \ldots, q \}^N} H(x)
  \]
  $x^*$ is a proper coloring when the minimum cost achieved is $0$. In the following section, we will describe how the Metropolis-Hastings algorithm can be used in this setting.

  \paragraph{Input data}
  In this project, we use Erd\"{o}s-R\'{e}nyi random graphs to test the correctness of our implementation and convergence speeds. They are constructed as follows: for a given density parameter $c$, each pair of the $N$ vertices is connected with probability $c / N$. Examples are given in Figure~\ref{Fig:random-graph-examples}, where an initial random coloring is also visualized.

  \begin{figure}[h]
      \centering
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-5-5.pdf}
        \caption{$c = 5$}
      \end{subfigure}
      \quad
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-10-5.pdf}
        \caption{$c = 10$}
      \end{subfigure}
      \quad
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-20-5.pdf}
        \caption{$c = 20$}
      \end{subfigure}

    \caption{Examples of Erd\"{o}s-R\'{e}nyi random graphs ($N = 50$ vertices) for different values of the density parameter $c$. We also visualize an initial random coloring: edges in bold represent a conflict between two vertices.}\label{Fig:random-graph-examples}
  \end{figure}

  % -----------------------------------------------------------------------------------
  \section*{MCMC solution}
  \paragraph{TODO} Describe how the optimization can be solved using Metropolis. Explain why we're convinced it will converge if there is a proper coloring.

  \paragraph{Simulated annealing}
  In order to find the global minimum of our cost function $H(x)$ that potentially contains several local minima, we make use of the \textit{simulated annealing}. This method simulates the physical process of cooling a solid such that when its structure is frozen, the minimum energy is reached. During the cooling procedure, we reduce the ``temperature'' $T$ or, equivalently, increase the ``inverse temperature'' $\beta=\frac{1}{T}$. The way the temperature is reduced is managed by a ``schedule'', a function of the current time step. In our implementation, we perform simulated annealing by increasing $\beta$ and the schedule will be referred to as $\beta(t)$.

  % -----------------------------------------------------------------------------------
  \section*{Results}
  \paragraph{Implementation details}
  TODO: Efficient implementations for generating the graph, transitioning, etc. Give some number to describe algorithm's performance (e.g. average time / iteration). Point out what can be parallelized (several independent chains with different initializations can run in parallel) and what cannot (a single chain).

  \paragraph{Experiments}  
  We define in Table \ref{tab:schedules} five different schedules, each tunable by some parameters. They all depend on the current time step $t$, proportionally to the maximum number of iterations. The inverse temperature should approach infinity when $t$ approaches the maximum number of iterations, such that the acceptance probabilities approach 0 at this time. [The adaptive schedule is similar to the exponential one, except that $\alpha$ is now a function of $t$ (for instance, $\alpha(t) = 1/t$).] Moreover, in each case, $t$ can be replaced by $t-\tau$ in order to keep the inverse temperature constant for a while before starting to increase it.
  
  TODO: Give specific numbers for each parameter (min / max temperature used, starting point, etc). Show results plot for each. Pick the best one and give intuition as to why it performs better than the others.
  
  \begin{table}[h]
    \begin{center}
    \def\arraystretch{1.5}
      \begin{tabular}{|c||c|c|c|c|c|}
      	\hline
        \textbf{Schedules} & \textbf{Linear} & \textbf{Sublinear}  & \textbf{Polynomial}  & \textbf{Logarithmic}& \textbf{Exponential} \\
        \hhline{|=||=|=|=|=|=|}
        \textbf{$\beta(t)$} & $\beta_0 + \alpha t$ & $\beta_0 t^{\alpha}$, $\alpha < 1$ & $\beta_0 t^{\alpha}$, $\alpha > 1$& $\beta_0 \log (t + \alpha)$, $\alpha \geq 1$ & $\beta_0\alpha^{-t}$, $0 < \alpha < 1$  \\
      	\hline
	\textbf{Parameters} & \begin{tabular}[t]{@{}c@{}}$\beta_0=0.001$ \\ $\alpha=6$\\$\tau=0.05$\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha= 0.5$\\$\tau=0$\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha=2.5$\\$\tau=0$\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha=1$\\$\tau=0$\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=0.01$ \\ $\alpha=0.0005$\\$\tau=0$\end{tabular} \\
	\hline
      \end{tabular}
    \end{center}
    \caption{Schedules for $\beta(t)$. The parameters are hand-tuned.}
    \label{tab:schedules}
  \end{table}

  We hand-tune the parameters of each schedule in order to obtain good results. The shape of the schedules are shown in Figure \ref{fig:schedules_shape}. We then compare them against each other on a graph of size $N=250$ and $q=15$ colors. The edges density $c$ is linearly increased from 0 to $N/4$. The results are displayed in Figure \ref{fig:schedules_evaluation}. The logarithmic schedule performs consistently better than the others.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{.5\linewidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/schedules_shape.pdf}
  \caption{Shape of the schedules described in Table \ref{tab:schedules}.}
  \label{fig:schedules_shape}
\end{subfigure}%
\begin{subfigure}[t]{.5\linewidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/schedules_evaluation.pdf}
  \caption{Results of the graph coloring on a graph of size $N=250$ and with $q=10$ colors. The logarithmic schedule performs the best.}
  \label{fig:schedules_evaluation}
\end{subfigure}
\caption{Shape of schedules and comparison against each other.}
\label{fig:schedules}
\end{figure}

  \begin{figure}
    \begin{center}
      \includegraphics[width=8cm]{figures/cost-vs-graph-density.pdf}
    \end{center}
    \label{Fig:cost-vs-density}
    \caption{Minimal energy achieved as a function of graph density $c$ for different values of $q$. Since both the input data and the process are random, we perform the optimization for $5$ random inputs for each set of parameters and show error bars.}
  \end{figure}

\end{document}

