\documentclass{article}

\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[table]{skip=0pt}
% \usepackage{url}
\usepackage{hyperref}
\usepackage{hhline}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{ bbold }
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\rbc}[1]{\textcolor{blue}{\small [#1]}} % For final version change to \newcommand{\rbc}[1]{}

\title{Simulated Annealing Algorithm for Graph Coloring\\Random Walks course project}
\date{May 25, 2016}

\author{
  R\'oger Berm\'udez Chac\'on\\EPFL
  \and
  Victor Kristof\\EPFL
  \and
  Merlin Nimier-David\\EPFL
}

\begin{document}
  \maketitle

  \paragraph{Abstract}
  We formulate the graph coloring problem as an optimization problem and use the Markov Chain Monte~Carlo method to find proper colorings. Simulated annealing is used to improve the convergence speed and avoid local minima. In this report, we recall the problem statement and formulation, describe the simulated annealing schemes devised and present results.

  % -----------------------------------------------------------------------------------
  \section*{Problem statement}
  \paragraph{Proper $q$-colorings}
  Let $G = (V, E)$ a graph with $|V| = N$ vertices. A \emph{coloring} using $q$ colors is an assignment $x \in \{ 1, \ldots, q \}^N$ for all elements in $V$. A \emph{proper} $q$-coloring is a coloring such that no two adjacent vertices have the same color: $(v, v') \in E \Rightarrow x_v \neq x_{v'}$. From this definition, we can define a cost function (Hamiltonian) to evaluate the ``quality'' of a given coloring:
  \[
    H(x) = \sum_{(v, v') \in E} \mathbb{1}_{x_v = x_{v'}}
  \]

  In the general case, finding the proper $q$-coloring of a graph of size $N$ is NP complete\footnote{\url{https://en.wikipedia.org/wiki/Graph_coloring\#Computational_complexity}}, assuming $2 < q < N$. The search space is the domain of all colorings $x$, which has a size exponential in $N$. It is therefore interesting to relax the problem into an optimization problem. Using the cost function defined above:
  \[
    x^* = \argmin_{x \in \{ 1, \ldots, q \}^N} H(x)
  \]
  We have, then, that $x^*$ is a proper coloring when the minimum cost achieved is $0$. In the following section, we will describe how the MCMC technique can be applied in this setting.

  \paragraph{Input data}
  In this project, we use Erd\H{o}s-R\'{e}nyi random graphs to test the correctness of our implementation and convergence speeds. They are constructed as follows: for a given density parameter $c$, each pair of the $N$ vertices is connected with probability $c / N$. Examples are given in Figure~\ref{Fig:random-graph-examples}, where an initial random coloring is also visualized.

  \begin{figure}[h]
      \centering
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-5-5.pdf}
        \caption{$c = 5$}
      \end{subfigure}
      \quad
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-10-5.pdf}
        \caption{$c = 10$}
      \end{subfigure}
      \quad
      \begin{subfigure}[t]{4.5cm}
        \centering
        \includegraphics[width=4.5cm]{figures/random-graph-50-20-5.pdf}
        \caption{$c = 20$}
      \end{subfigure}

    \caption{Examples of Erd\H{o}s-R\'{e}nyi random graphs ($N = 50$ edges) for different values of the density parameter $c$. We also visualize an initial random coloring: numbers next to the edges represent their colors; improper vertex colorings are shown in bold.}\label{Fig:random-graph-examples}
  \end{figure}

  % -----------------------------------------------------------------------------------
  \section*{MCMC solution}
  Once the graph coloring problem is formulated as an optimization problem, we are able to bring it into the MCMC framework. We setup a simple Markov Chain over the space of colorings. Transitions correspond to changing one vertex (chosen uniformly at random) to a different color (chosen uniformly at random). After generating a proposed transition, we compute the change in cost:
  \[
    \Delta = H(x^{new}) - H(x^t)
  \]
  The proposed transition is performed with some acceptance probability, defined below. This process is repeated until a proper coloring is found ($H(x) = 0$) or a maximum number of iterations is reached.

  Figure~\ref{Fig:random-walk-example} shows the evolution of cost $H(x)$ over the course of an example walk. When enough colors $q$ are available w.r.t. the density $c$ of the graph, the walk converges to a proper coloring in few iterations, which run fast even on a commodity laptop: for $N \approx 1000$, the search completes $5000$ iterations in less than 3 minutes. Note that most of the computation time dedicated to the evaluation of the cost function. We plot in Figure~\ref{Fig:cost-vs-density} the relationship between the best cost achieved and the graph's density, for different values of $q$.

  \begin{figure}[h]
    \centering
    \begin{subfigure}[t]{.49\linewidth}
      \centering
      \includegraphics[width=.8\linewidth]{figures/random-walk-example.pdf}
      \caption{}\label{Fig:random-walk-example}
    \end{subfigure}
    \begin{subfigure}[t]{.49\linewidth}
      \centering
      \includegraphics[width=.8\linewidth]{figures/cost-vs-graph-density.pdf}
      \caption{}\label{Fig:cost-vs-density}
    \end{subfigure}
    \caption{Left: evolution of the cost over time for an example graph. Right: minimal energy achieved as a function of graph density $c$ for different values of $q$ under a sublinear simulated annealing schedule. Since both the input data and the process are random, we perform the optimization for $5$ random inputs for each set of parameters and show error bars.}
  \end{figure}

  \paragraph{Simulated annealing}
  In order to find the global minimum of our cost function $H(x)$, which potentially contains several local minima, we make use of the \textit{simulated annealing} metaheuristic. This method simulates the physical process of applying temperature to a metal and then progressively cooling it down, in order to achieve a crystal configuration with lower energy than the original, which improves some properties of the material. During the cooling procedure, we reduce the ``temperature'' $T$ or, equivalently, increase the ``inverse temperature'' $\beta=\frac{1}{T}$. The way the temperature is reduced is managed by a \emph{schedule}, a function of the current time step. In our implementation, we perform simulated annealing by increasing $\beta$ and the schedule will be referred to as $\beta(t)$. The acceptance probability for a proposed move with energy difference $\Delta$ is defined as:
  \[
    a = \exp(- \beta(t) \cdot \Delta)
  \]

  % -----------------------------------------------------------------------------------
  \section*{Results}
  % This paragraph is not so important, since we don't have a lot of space
  % \paragraph{Implementation details}
  % TODO: Efficient implementations for generating the graph, transitioning, etc. Give some number to describe algorithm's performance (e.g. average time / iteration). Point out what can be parallelized (several independent chains with different initializations can run in parallel) and what cannot (a single chain).

  % \paragraph{Experiments}
  We propose in Table~\ref{tab:schedules} five different schedules, each adjustable by two parameters. Schedules depend mainly on the current time step $t \in [0, 1]$, which is normalized by the maximum number of iterations. Generally, the inverse temperature $\beta(t)$ should approach ``infinity'' when the simulation approaches its maximum number of iterations, so that the acceptance probability of an error-increasing transition tends to $0$. As a variation, we create a discretized version of each schedule: the value of $\beta$ is kept constant for some number of iterations before jumping discretely to the next value in the schedule, as illustrated on Figure~\ref{fig:schedules_discretized_shape}.

  \begin{table}[h]
    \begin{center}
    \def\arraystretch{1.5}
      \begin{tabular}{|c||c|c|c|c|c|}
      	\hline
        \textbf{Schedules} & \textbf{Linear} & \textbf{Sublinear}  & \textbf{Polynomial}  & \textbf{Logarithmic}& \textbf{Exponential} \\
        \hhline{|=||=|=|=|=|=|}
        \textbf{$\beta(t)$} & $\beta_0 + \alpha t$ & $\beta_0 t^{\alpha}$, $\alpha < 1$ & $\beta_0 t^{\alpha}$, $\alpha > 1$& $\beta_0 \log (t + \alpha)$, $\alpha \geq 1$ & $\beta_0\alpha^{-t}$, $0 < \alpha < 1$  \\
      	\hline
	\textbf{Parameters} & \begin{tabular}[t]{@{}c@{}}$\beta_0=0.001$ \\ $\alpha=6$\\\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha= 0.5$\\\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha=2.5$\\\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=10$ \\ $\alpha=1$\\\end{tabular} &  \begin{tabular}[t]{@{}c@{}}$\beta_0=0.01$ \\ $\alpha=0.0005$\\\end{tabular} \\
	\hline
      \end{tabular}
    \end{center}
    \caption{Proposed simulated annealing schedules $\beta(t)$. The parameter values were determined experimentally. The schedules can be visualized on Figure~\ref{fig:schedules_linear_shape} (continuous version) and \ref{fig:schedules_discretized_shape} (discretized version).}
    \label{tab:schedules}
  \end{table}

  After selecting values for each parameter experimentally, we compare them against each other on a graph of size $N=250$ and using $q=10$ colors. The edges density $c$ is linearly increased from $0$ to $N/4$. Each discretized schedules performs better than its continuous counterpart. Even though the differences are not very marked, the continuous sublinear schedule (here, a square root) is the best continuous schedule. It is compared against the discretized schedules on Figure~\ref{fig:schedules_evaluation}. We notice that the logarithmic schedule, also a slow-growing function of time, achieves similar performance. We expect that further gains can be achieved with careful tuning of the parameters and experimentation with other variations.

  \begin{figure}[h]
    \centering
    \begin{subfigure}[t]{.33\linewidth} % Linear
      \centering
      \includegraphics[width=1\linewidth]{figures/schedules_linear_shape.pdf}
      \caption{Schedules described in Table \ref{tab:schedules}.}
      \label{fig:schedules_linear_shape}
    \end{subfigure}%
     \begin{subfigure}[t]{.33\linewidth} % Discretized
      \centering
      \includegraphics[width=1\linewidth]{figures/schedules_discretized_shape.pdf}
      \caption{Discretized schedules.}
      \label{fig:schedules_discretized_shape}
    \end{subfigure}%
    \begin{subfigure}[t]{.33\linewidth} % Evaluation
      \centering
      \includegraphics[width=1\linewidth]{figures/schedules_evaluation.pdf}
      \caption{Comparing the performance of discretized schedules and the ``sublinear'' continuous schedule for various inputs.}
      \label{fig:schedules_evaluation}
    \end{subfigure}
    \caption{Shape of schedules and comparison against each other.}
    \label{fig:schedules}
  \end{figure}


  % -----------------------------------------------------------------------------------
  \section*{Conclusion}
  In this project, we formulated the graph coloring problem as an optimization problem and applied the MCMC technique to minimize it. Simulated annealing is used to avoid local minima of the Hamiltonian. Our implementation handles inputs of size $N=1000$ run over 5000 iterations in less than 3 minutes on a commodity laptop. We compared several schedules to decrease the temperature and found that discretized schedules perform better than most continuous ones. The sublinear continuous schedule provide results that are comparable to the discretized ones. We expect that even better performance can be achieved by tuning the parameters further.

  \rbc{So which schedule are we recommending as the best?}

\end{document}

